{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Question Analysis\n",
    "\n",
    "The goal of this assignment is to write a more flexible version of the interactive QA system. As in the previous assignment, the system should be able to take a question in natural language (English) as input, analyse the question, and generate a SPARQL query for it.\n",
    "\n",
    "## Assignment  // Additional requirements\n",
    "\n",
    "* Make sure that your system can analyse at least two more question types. E.g. questions that start with *How*, *When*, or where the property is expressed by a verb, yes/no questions, etc.\n",
    "* Apart from the techniques introduced last week (matching tokens on the basis of their lemma or part-of-speech), also include at least one pattern where you use the dependency relations to find the relevant property or entity in the question. \n",
    "* Include 10 examples of questions that your system can handle, and that illustrate the fact that you cover additional question types\n",
    "\n",
    "## Examples\n",
    "\n",
    "Here is a non-representative list of questios and question types to consider. See the list with all questions for more examples\n",
    "\n",
    "* What do cows eat?\n",
    "* How old can a European hedgehog get?\n",
    "* Which tree produces papayas?\n",
    "* How fast can a cheetah run?\n",
    "* When did dodos go extinct?\n",
    "* Where do platypuses live?\n",
    "* Is a lion a feline?\n",
    "* Can you eat ferns?\n",
    "* How many beats per minutes does a chicken have at rest?\n",
    "* Who gave the Canis lupus familiaris its name?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\") # this loads the model for analysing English text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Analysis with Spacy\n",
    "\n",
    "All the functionality of Spacy, as in the last assignment, is still available for doing question analysis. \n",
    "\n",
    "In addition, also use the dependency relations assigned by spacy. Note that a dependency relation is a directed, labeled, arc between two tokens in the input. In the example below, the system detects that *movie* is the subject of the passive sentence (with label nsubjpass), and that the head of which this subject is a dependent is the word *are* with lemma *be*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Where do platypuses live?'\n",
    "# nlp = spacy.load('en_core_sci_lg')\n",
    "parse = nlp(question) # parse the input\n",
    "\n",
    "print(list(parse.noun_chunks))\n",
    "# print(getWikidataIDs(\"live\", True))\n",
    "\n",
    "for word in parse : # iterate over the token objects \n",
    "    print(word.lemma_, word.pos_, word.dep_, word.head.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases\n",
    "\n",
    "You can also match with the full phrase that is the subject of the sentence, or any other dependency relation, using the subtree function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase(word) :\n",
    "    children = []\n",
    "    for child in word.subtree :\n",
    "        children.append(child.text)\n",
    "    return \" \".join(children)\n",
    "\n",
    "for word in parse:\n",
    "    if word.dep_ == 'nsubjpass' or word.dep_ == 'agent' :\n",
    "        phrase_text = phrase(word)\n",
    "        print(phrase_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "For a quick understanding of what the parser does, and how it assigns part-of-speech, entities, etc. you can also visualise parse results. Below, the entity visualiser and parsing visualiser is demonstrated. \n",
    "This code is for illustration only, it is not part of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "parse = nlp(\"Which tree produces papayas?\")\n",
    "\n",
    "displacy.render(parse, jupyter=True, style=\"ent\")\n",
    "\n",
    "displacy.render(parse, jupyter=True, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, spacy\n",
    "\n",
    "def formatAnswer(data):\n",
    "    data = data[0]\n",
    "    ans = str(data['answerLabel']['value']).capitalize()\n",
    "\n",
    "    unit = ''\n",
    "    if 'unitLabel' in data:\n",
    "        unit =  ' ' + data['unitLabel']['value']\n",
    "\n",
    "    # If the answer is a (number > 1) + unit combination, make unit plural\n",
    "    if ans.replace('.','',1).isdigit() and float(ans) > 1.0:\n",
    "       unit += 's'\n",
    "\n",
    "    return ans + unit\n",
    "\n",
    "\n",
    "\n",
    "def queryWikidata(query) :\n",
    "    try:\n",
    "        data = requests.get('https://query.wikidata.org/sparql',\n",
    "            headers = {'User-Agent': 'QASys/0.0 (https://rug.nl/LTP/; josh@bruegger.it)'},\n",
    "            params={'query': query, 'format': 'json'}).json()['results']['bindings']\n",
    "    except:\n",
    "        return 'Error: Query failed: Too many requests?'\n",
    "\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "\n",
    "    return formatAnswer(data)\n",
    "\n",
    "\n",
    "\n",
    "def buildQuery(object, property):\n",
    "    q = 'SELECT ?answerLabel ?unitLabel WHERE{wd:'\n",
    "    q += object + ' p:' + property + '?s.?s ps:'\n",
    "    q += property + '?answer. OPTIONAL{?s psv:'\n",
    "    q += property + '?u.?u wikibase:quantityUnit ?unit.}'\n",
    "    q += 'SERVICE wikibase:label{bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\"}}'\n",
    "    return q\n",
    "\n",
    "\n",
    "\n",
    "def getWikidataIDs(query, isProperty = False):\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    headers = {'User-Agent': 'QASys/0.0 (https://rug.nl/LTP/); josh@bruegger.it)'}\n",
    "\n",
    "    params = {'action':'wbsearchentities',\n",
    "            'language':'en',\n",
    "            'format':'json',\n",
    "            'limit':'50',\n",
    "            'search': query}\n",
    "    if isProperty:\n",
    "        params['type'] = 'property'\n",
    "\n",
    "    return requests.get(url,params, headers = headers).json()['search']\n",
    "\n",
    "\n",
    "\n",
    "def extractPropertyAndObject(question):\n",
    "    nlp = spacy.load(\"en_core_web_sm\") # this loads the model for analysing English text\n",
    "    doc = nlp(question) # parse the input\n",
    "\n",
    "    nouns = list(doc.noun_chunks)\n",
    "    removeArticle = lambda str: re.sub('^(?:the|a|an) ', '', str)\n",
    "\n",
    "    if len(nouns) < 3:\n",
    "        print(len(nouns))\n",
    "        return \"ERR\", \"ERR\"\n",
    "\n",
    "    object = removeArticle(nouns[len(nouns)-1].text)\n",
    "    if (len(nouns) > 3):\n",
    "        pattern = '(' + nouns[1].text + '.*?' + nouns[len(nouns)-2].text + ')'\n",
    "        m = re.search(pattern, question)\n",
    "        property = removeArticle(m.group(1))\n",
    "    else:\n",
    "        property = removeArticle(nouns[1].text)\n",
    "\n",
    "    return property, object\n",
    "\n",
    "\n",
    "\n",
    "# Has to match (?:Who|What) (?:was|is|were) ?(?:the|a|an)? (?:.+) of ?(?:the|a|an)? (?:.+)\\?\n",
    "def getAnswer(question):\n",
    "    propertyText, objectText = extractPropertyAndObject(question)\n",
    "\n",
    "    #print('Looking for: ' + propertyText + ' of ' + objectText + '...')\n",
    "\n",
    "    possibleObjects = getWikidataIDs(objectText)\n",
    "    possibleProperties = getWikidataIDs(propertyText, True)\n",
    "\n",
    "    for object in possibleObjects:\n",
    "        for property in possibleProperties:\n",
    "            #print('trying: ' + property['id'] + ' of ' + object['id'])\n",
    "            answer = queryWikidata(buildQuery(object['id'], property['id']))\n",
    "            if answer is not None:\n",
    "                return answer\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def answerQuestion(q):\n",
    "    ans = getAnswer(q)\n",
    "    print('Question: ' + q)\n",
    "    if (ans != None):\n",
    "        print('Answer: ' + ans)\n",
    "    else:\n",
    "        print(\"Sorry, I don't know!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sequenced genome URL', 'video', 'award received', 'parent taxon', 'taxon name', 'pronunciation audio', 'taxon common name', 'Commons gallery', 'Commons category', 'image', 'on focus list of Wikimedia project', 'taxon range map image', 'IUCN conservation status', \"topic's main category\", 'gestation period', 'diel cycle', 'instance of', 'taxon rank', 'NBN System Key', 'EPPO Code', 'ITIS TSN', 'highest observed lifespan']\n",
      "0.5833975185661963 : on focus list of Wikimedia project\n",
      "0.5758644747240347 : instance of\n",
      "0.5558329773388655 : topic's main category\n",
      "0.47577313833952456 : video\n",
      "0.40596640635175096 : taxon common name\n",
      "0.40422444163639043 : taxon range map image\n",
      "0.39444809488485993 : highest observed lifespan\n",
      "0.3905310878629736 : image\n",
      "0.3770107429114158 : diel cycle\n",
      "0.3622714463190898 : EPPO Code\n",
      "0.35922744031450804 : Commons gallery\n",
      "0.34479242451065223 : pronunciation audio\n",
      "0.31448513764637925 : award received\n",
      "0.30480809228589606 : gestation period\n",
      "0.29195863626017393 : Commons category\n",
      "0.28817702755829905 : NBN System Key\n",
      "0.28187886721370775 : sequenced genome URL\n",
      "0.23622988386795588 : taxon name\n",
      "0.20764814828592995 : parent taxon\n",
      "0.14761860037680238 : taxon rank\n",
      "0.1311283161839377 : IUCN conservation status\n",
      "-0.03707295060519356 : ITIS TSN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import classy_classification\n",
    "\n",
    "query = '''SELECT DISTINCT ?wdLabel WHERE {\n",
    "  wd:Q6145 ?wdt ?a.\n",
    "  ?wd wikibase:directClaim ?wdt .\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}'''\n",
    "\n",
    "try:\n",
    "  data = requests.get('https://query.wikidata.org/sparql',\n",
    "                      headers = {'User-Agent': 'QASys/0.0 (https://rug.nl/LTP/; josh@bruegger.it)'},\n",
    "                      params={'query': query, 'format': 'json'}).json()['results']['bindings']\n",
    "except:\n",
    "  print('Error: Query failed: Too many requests?')\n",
    "\n",
    "\n",
    "properties = []\n",
    "for item in data:\n",
    "  p = item['wdLabel']['value']\n",
    "  if 'ID' not in p:\n",
    "    properties.append(p)\n",
    "\n",
    "print(properties)\n",
    "\n",
    "question = \"How old can a European hedgehog get?\"\n",
    "qDoc = nlp(question)\n",
    "\n",
    "sim = {}\n",
    "\n",
    "for p in properties:\n",
    "  pDoc = nlp(p)\n",
    "  # print(question + ' : ' + p + '       ' + str(pDoc.similarity(qDoc)))\n",
    "  sim[pDoc.similarity(qDoc)] = p\n",
    "\n",
    "#print map by sorting keys\n",
    "for k in sorted(sim.keys(), reverse=True):\n",
    "  print(str(k) + ' : ' + sim[k])\n",
    "\n",
    "# text1 = 'How can I kill someone?'\n",
    "# text2 = 'What should I do to be a peaceful?'\n",
    "# doc1 = nlp(text1)\n",
    "# doc2 = nlp(text2)\n",
    "# print(\"spaCy :\", doc1.similarity(doc2))\n",
    "\n",
    "\n",
    "# nlp = spacy.blank(\"en\")\n",
    "# nlp.add_pipe(\n",
    "#     \"text_categorizer\",\n",
    "#     config={\n",
    "#         \"data\": properties,\n",
    "#         \"model\": \"facebook/bart-large-mnli\",\n",
    "#         # \"model\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "#         \"cat_type\": \"zero\",\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# print(nlp(question)._.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy : 0.9035856671380171\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "text1 = 'How can I kill someone?'\n",
    "text2 = 'What should I do to be a peaceful?'\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "print(\"spaCy :\", doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the life expectancy of a cat?\n",
      "()\n",
      "[What, the life expectancy, a cat]\n",
      "How long is the gestation period of the European rabbit?\n",
      "(European,)\n",
      "[the gestation period, the European rabbit]\n",
      "Who was the polar bear born in captivity at the Berlin Zoological Garden in 2006?\n",
      "(the Berlin Zoological Garden, 2006)\n",
      "[Who, the polar bear, captivity, the Berlin Zoological Garden]\n",
      "How many Chinese common names are there for a great white shark?\n",
      "(Chinese,)\n",
      "[How many Chinese common names, a great white shark]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "data = json.load(open('../all_questions.json'))\n",
    "\n",
    "for ex in data[1:5]:\n",
    "    print(ex['string'])\n",
    "    doc = nlp(ex['string'])\n",
    "    nouns = list(doc.noun_chunks)\n",
    "    print(doc.ents)\n",
    "    print(nouns)\n",
    "    # for ent in doc.ents :\n",
    "    #     print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_sci_lg')\n",
    "\n",
    "question = \"What is the life expectancy of a cat?\"\n",
    "\n",
    "#Build wikidata search query from question\n",
    "from spacy.tokens import Span\n",
    "# wikidata_id is the attribute we add to the parse results\n",
    "# wikidata_entity_link is the function that calls the api\n",
    "# Span.set_extension('wikidata_id',getter=getWikidataIDs)\n",
    "\n",
    "doc = nlp(question)\n",
    "\n",
    "firstWord = doc[0].text.lower()\n",
    "match firstWord:\n",
    "    case 'what':\n",
    "        print('what')\n",
    "    case \"is\" |'does' | 'are' | 'do' | 'can':\n",
    "        print('is')\n",
    "    case \"where\":\n",
    "        print('where')\n",
    "    case \"which\":\n",
    "        print('which')\n",
    "    case _:\n",
    "        print(\"IDK\")\n",
    "# for ent in doc.ents :\n",
    "#     print(ent.text, ent._.wikidata_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPropertyAndObject(question):\n",
    "    nlp = spacy.load(\"en_core_web_sm\") # this loads the model for analysing English text\n",
    "    doc = nlp(question) # parse the input\n",
    "\n",
    "    nouns = list(doc.noun_chunks)\n",
    "    removeArticle = lambda str: re.sub('^(?:the|a|an) ', '', str)\n",
    "\n",
    "    if len(nouns) < 3:\n",
    "        print(len(nouns))\n",
    "        return \"ERR\", \"ERR\"\n",
    "\n",
    "    object = removeArticle(nouns[len(nouns)-1].text)\n",
    "    if (len(nouns) > 3):\n",
    "        pattern = '(' + nouns[1].text + '.*?' + nouns[len(nouns)-2].text + ')'\n",
    "        m = re.search(pattern, question)\n",
    "        property = removeArticle(m.group(1))\n",
    "    else:\n",
    "        property = removeArticle(nouns[1].text)\n",
    "\n",
    "    return property, object"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
