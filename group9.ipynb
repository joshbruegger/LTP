{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written by group 9:\n",
    "- Zeynep D. Metin (s4236599) - How and When questions\n",
    "- Melisa Samancioglu (s4010418) - Which and What questions\n",
    "- Mara Nedelcu (s4245350) - When and Where questions\n",
    "- Josh Bruegger (s4361687) - 'Yes/No' and 'how many' questions\n",
    "\n",
    "Everyone helped each other in the process of writing this notebook.\n",
    "\n",
    "This program requires nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2JSlayQA 6.9\n",
      "-----------\n",
      "\n",
      "    1. Ask a question\n",
      "    2. try all questions in file\n",
      "    3. Turn debug on or off\n",
      "    q. Quit\n",
      "    \n",
      "A lion is a species of big cat\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import traceback\n",
    "from cmath import e\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# download only if not already downloaded\n",
    "if not nltk.data.find('corpora/wordnet') or not nltk.data.find('corpora/omw-1.4'):\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    print(\"=============================================================================================================================\")\n",
    "\n",
    "\n",
    "PRINT_DEBUG = False\n",
    "SPARQL_ENDPOINT = 'https://query.wikidata.org/sparql'\n",
    "API_ENDPOINT = 'https://www.wikidata.org/w/api.php'\n",
    "HEADERS = {'User-Agent': 'QASys/0.0 (https://rug.nl/LTP/; dont@mail.me)'}\n",
    "\n",
    "SPACY_MODEL = 'en_core_web_trf'\n",
    "NLP = spacy.load(SPACY_MODEL)\n",
    "\n",
    "YES_NO_LIST = [\"is\", \"are\", \"can\", \"do\", \"does\"]\n",
    "\n",
    "\n",
    "def log(msg):\n",
    "    if PRINT_DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def clear_console(): return os.system(\n",
    "    'cls' if os.name in ('nt', 'dos') else 'clear')\n",
    "\n",
    "\n",
    "def try_all_questions(file):\n",
    "    f = open('questions/' + file)\n",
    "    questions = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    manual_check_choice = input(\"Manual check? (y/n): \")\n",
    "    while (manual_check_choice != 'y' and manual_check_choice != 'n'):\n",
    "        manual_check_choice = input(\"Manual check? (y/n): \")\n",
    "\n",
    "    n_correct = 0\n",
    "    n_tot = 0\n",
    "    incorrect = []\n",
    "\n",
    "    for question in questions:\n",
    "        print(\"=============================================================================================================================\")\n",
    "        n_tot += 1\n",
    "\n",
    "        q = question.get('question')\n",
    "        print(q)\n",
    "        print(\"expected answer: \" + question.get('expected'))\n",
    "        ans = answer(q)\n",
    "        if manual_check_choice == 'y':\n",
    "            choice = input(\"Correct? (y/n): \")\n",
    "            while (choice != 'y' and choice != 'n'):\n",
    "                choice = input(\"Correct? (y/n): \")\n",
    "            if choice == 'y':\n",
    "                n_correct += 1\n",
    "            else:\n",
    "                incorrect.append([q, question.get('expected'), ans])\n",
    "        else:\n",
    "            if ans == question.get('expected'):\n",
    "                n_correct += 1\n",
    "                print(\"Correct!\")\n",
    "            else:\n",
    "                print(\"Incorrect!\")\n",
    "                incorrect.append([q, question.get('expected'), ans])\n",
    "\n",
    "        print(\"Current accuracy: \" + str(n_correct / n_tot))\n",
    "\n",
    "    print(\"=============================================================================================================================\")\n",
    "    print(\"Score: \" + str(n_correct) + \"/\" + str(n_tot))\n",
    "    print(\"Accuracy: \" + str(n_correct / n_tot))\n",
    "\n",
    "    print(\"Incorrect questions:\")\n",
    "    for i in incorrect:\n",
    "        print(i[0])\n",
    "        print(\"Expected: \" + i[1])\n",
    "        print(\"Actual: \" + answer(i[2]))\n",
    "\n",
    "\n",
    "def questions_from_selection():\n",
    "    print('''\n",
    "    1. how questions\n",
    "    2. when questions\n",
    "    3. where questions\n",
    "    4. which questions\n",
    "    5. yes/no questions\n",
    "    6. what questions\n",
    "    ''')\n",
    "    choice = input(\"Enter your choice: \")\n",
    "    if choice == '1':\n",
    "        try_all_questions('how_questions.json')\n",
    "    elif choice == '2':\n",
    "        try_all_questions('when_questions.json')\n",
    "    elif choice == '3':\n",
    "        try_all_questions('where_questions.json')\n",
    "    elif choice == '4':\n",
    "        try_all_questions('which_questions.json')\n",
    "    elif choice == '5':\n",
    "        try_all_questions('yes_no_questions.json')\n",
    "    elif choice == '6':\n",
    "        try_all_questions('what_questions.json')\n",
    "    else:\n",
    "        print(\"Invalid choice\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    clear_console()\n",
    "\n",
    "    print(\"SlayQA 6.9\")\n",
    "    # while True:\n",
    "    print('-----------')\n",
    "    print('''\n",
    "    1. Ask a question\n",
    "    2. try all questions in file\n",
    "    3. Turn debug on or off\n",
    "    q. Quit\n",
    "    ''')\n",
    "    choice = input(\"Enter your choice: \")\n",
    "    if choice == '1':\n",
    "        question = input(\"Enter your question: \")\n",
    "        answer(question)\n",
    "    elif choice == '2':\n",
    "        questions_from_selection()\n",
    "    elif choice == '3':\n",
    "        global PRINT_DEBUG\n",
    "        PRINT_DEBUG = not PRINT_DEBUG\n",
    "        print(\"Debug is now \" + str(PRINT_DEBUG))\n",
    "    elif choice == 'q':\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"Invalid choice\")\n",
    "\n",
    "\n",
    "def answer(question):\n",
    "    question = question.replace(\"?\", \"\")\n",
    "\n",
    "    doc = NLP(question)  # parse the input\n",
    "\n",
    "    first_word = doc[0].text.lower()  # get the first word\n",
    "\n",
    "    if \"what\" in doc.text.lower():\n",
    "        ans = what_question(doc)\n",
    "    elif first_word in YES_NO_LIST:\n",
    "        ans = yes_no_q(doc)\n",
    "    elif (\"where\" in doc.text.lower()):\n",
    "        ans = where_question(doc)\n",
    "    elif \"which\" in doc.text.lower() or \"who\" in doc.text.lower():\n",
    "        ans = which_question(doc)\n",
    "    elif \"when\" in doc.text.lower():\n",
    "        ans = when_q(doc)\n",
    "    elif \"how\" in doc.text.lower():\n",
    "        ans = how_q(doc)\n",
    "    else:\n",
    "        ans = None\n",
    "\n",
    "    if ans is not None:\n",
    "        print(ans)\n",
    "        return ans\n",
    "    else:\n",
    "        print('Answer: I don\\'t know')\n",
    "        return None\n",
    "\n",
    "\n",
    "def remove_article(text):\n",
    "    words_to_remove = ['the', 'a', 'an', 'while',\n",
    "                       'which', 'who', 'what', 'where', 'when', 'how']\n",
    "\n",
    "    for word in words_to_remove:\n",
    "        # remove beginnings\n",
    "        text = re.sub('^' + word + ' ', '', text)\n",
    "        text = re.sub(' ' + word + ' ', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_adj(str):\n",
    "    ret = \"\"\n",
    "    for word in str:\n",
    "        if word.pos_ != 'ADJ':\n",
    "            ret = ret + \" \" + word.text\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(str(word), pos=\"n\"):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(str(l.name()))\n",
    "    return synonyms\n",
    "\n",
    "\n",
    "def what_question(doc):\n",
    "    nouns = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        nouns.append(str(chunk))\n",
    "\n",
    "    to_remove = ['average', 'common', 'mean', 'typical']\n",
    "    completey_remove = ['bpm']\n",
    "    # substitute word in all nouns with an empty string\n",
    "    for noun in nouns:\n",
    "        # if noun is not in to_remove:\n",
    "        if noun in completey_remove:\n",
    "            nouns.remove(noun)\n",
    "        else:\n",
    "            for word in to_remove:\n",
    "                if word in noun:\n",
    "                    nouns.remove(noun)\n",
    "                    noun = re.sub(word + ' ', '', noun)\n",
    "                    nouns.append(noun)\n",
    "\n",
    "    log('Noun chunks: ' + str(nouns))\n",
    "\n",
    "    if len(nouns) < 3:\n",
    "        # If there are less than 2 noun chunks, it's probably a question like \"What is a lion?\"\n",
    "        entity = wnl.lemmatize(remove_article(nouns[1]))\n",
    "        if (\"also\" in doc.text):\n",
    "            possible_objects = get_wikidata_ids(entity)\n",
    "            answer = query_wikidata(build_label_query(\n",
    "                possible_objects[0]['id']))\n",
    "            if answer is not None:\n",
    "                return format_answer(answer)\n",
    "        else:\n",
    "            possible_objects = get_wikidata_ids(entity)\n",
    "            for obj in possible_objects:\n",
    "                desc = obj['display']['description']['value']\n",
    "                if desc is not None:\n",
    "                    # check if entity starts with vowel\n",
    "                    if entity[0].lower() in 'aeiou':\n",
    "                        entity = \"An \" + entity\n",
    "                    else:\n",
    "                        entity = \"A \" + entity\n",
    "                    return entity + ' is a ' + desc\n",
    "\n",
    "    else:\n",
    "        entity = wnl.lemmatize(remove_article(nouns[-1]))\n",
    "\n",
    "        if (len(nouns) == 3):\n",
    "            prop = remove_article(nouns[1])\n",
    "\n",
    "        # Checks if the question has \"another word for/other names of\" or else, and queries using \"skos:altLabel\"\n",
    "\n",
    "        if (str(nouns[1]) in diffName):\n",
    "            log(\"Property: \" + prop + \"Entity: \" + entity)\n",
    "            possible_objects = get_wikidata_ids(entity)\n",
    "            answer = query_wikidata(build_label_query(\n",
    "                possible_objects[0]['id']))\n",
    "            if answer is not None:\n",
    "                return format_answer(answer)\n",
    "\n",
    "        else:\n",
    "            prop = remove_article(nouns[1])\n",
    "\n",
    "    ans = solver(entity, prop)\n",
    "    if ans is not None:\n",
    "        return ans\n",
    "    return solver(prop, entity)\n",
    "\n",
    "\n",
    "def where_question(doc):\n",
    "    nouns = list(doc.noun_chunks)\n",
    "    nouns = process_noun(nouns)\n",
    "\n",
    "    log('Noun chunks: ' + str(nouns))\n",
    "\n",
    "    entity = wnl.lemmatize(remove_article(process_noun(nouns[0].text)))\n",
    "\n",
    "    if doc[-1].text == 'discovered':\n",
    "        property1 = \"location of discovery\"\n",
    "    else:\n",
    "        property1 = \"endemic to\"\n",
    "\n",
    "    property2 = \"country of origin\"\n",
    "    property1 = process_noun(property1)\n",
    "    property2 = process_noun(property2)\n",
    "\n",
    "    ans = None\n",
    "    ans = solver(entity, property1)\n",
    "    if ans is None:\n",
    "        ans = solver(entity, property2)\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "def which_question(doc):\n",
    "    nouns = []\n",
    "    ans = None\n",
    "    words = ''\n",
    "\n",
    "    for word in doc.noun_chunks:\n",
    "        words = remove_article(word.text.lower())\n",
    "        log('words: ' + str(words))\n",
    "        words = wnl.lemmatize(words)\n",
    "        words = process_noun(words)\n",
    "        if len(words.split()) > 1:\n",
    "            treeCheck = words.split()\n",
    "            if treeCheck[-1] == 'tree' and treeCheck[0] != treeCheck[-1]:\n",
    "                words = words.rsplit(' ', 1)[0]\n",
    "        nouns += [words]\n",
    "\n",
    "    nouns = list(filter(None, nouns))\n",
    "    log('Nouns: ' + str(nouns))\n",
    "\n",
    "    if (len(nouns)) == 2:\n",
    "        ans = solver(nouns[1], nouns[0])\n",
    "    elif (len(nouns)) > 2:\n",
    "        ans = solver(nouns[-1], nouns[1])\n",
    "        if ans is None:\n",
    "            ans = solver(nouns[1], nouns[-1])\n",
    "\n",
    "    for word in doc:\n",
    "        if (ans is None and word.text.lower() not in nouns and\n",
    "            (word.pos_ == \"NOUN\" or word.pos_ == \"ADJ\" or\n",
    "             word.pos_ == \"ADV\" or word.pos_ == \"VERB\")):\n",
    "            words = remove_article(word.text.lower())\n",
    "            words = process_noun(words)\n",
    "            ans = solver(nouns[-1], words)\n",
    "            if ans is None:\n",
    "                ans = solver(nouns[0], words)\n",
    "                if ans is None:\n",
    "                    ans = solver(words, nouns[0])\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "def yes_no_q(doc):\n",
    "    log('Yes/No question')\n",
    "\n",
    "    nouns = []\n",
    "    adj_verb = None\n",
    "\n",
    "    # Find the nouns in the question and the adjective/verb\n",
    "    for word in doc:\n",
    "        if word.pos_ == \"ADJ\" or word.pos_ == \"VERB\":\n",
    "            adj_verb = word.text\n",
    "        if word.pos_ == \"NOUN\":\n",
    "            lemma = wnl.lemmatize(word.text)  # lemmatize the word\n",
    "            nouns.append(lemma)\n",
    "\n",
    "    log('Noun chunks: ' + str(nouns))\n",
    "    if adj_verb is not None:\n",
    "        log('Adjective/Verb: ' + adj_verb)\n",
    "    else:\n",
    "        log('No adjective found')\n",
    "\n",
    "    if len(nouns) == 0:\n",
    "        log('No nouns found!')\n",
    "        return None\n",
    "\n",
    "    if len(nouns) == 1:\n",
    "        log('Only one noun found, trying with the adjective/verb')\n",
    "        if adj_verb is None:\n",
    "            log('No adjective/verb found')\n",
    "            return None\n",
    "\n",
    "        possible_objects = get_wikidata_ids(nouns[0])\n",
    "        for obj in possible_objects:\n",
    "            log('finding: ' + adj_verb + ' in ' +\n",
    "                obj['display']['label']['value'] + '...')\n",
    "            data = str(query_wikidata(query_all_properties(obj['id'])))\n",
    "            if adj_verb in data:\n",
    "                log('Found!')\n",
    "                return \"Yes\"\n",
    "\n",
    "        log(\"Not found\")\n",
    "        return \"No\"\n",
    "\n",
    "    noun1_ids = get_wikidata_ids(nouns[0])\n",
    "    noun2_ids = get_wikidata_ids(nouns[1])\n",
    "    for noun1_id in noun1_ids:\n",
    "        for noun2_id in noun2_ids:\n",
    "            log('trying: ' + noun2_id['display']['label']['value'] +\n",
    "                ' in ' + noun1_id['display']['label']['value'])\n",
    "\n",
    "            noun1_data = query_wikidata(query_all_properties(noun1_id['id']))\n",
    "            noun2_data = query_wikidata(query_all_properties(noun2_id['id']))\n",
    "            if noun2_id['display']['label']['value'] in str(noun1_data) or noun1_id['display']['label']['value'] in str(noun2_data):\n",
    "                different_from_text = 'different from'\n",
    "                log('found references')\n",
    "                if different_from_text in str(noun1_data):\n",
    "                    log('Different from in n1')\n",
    "                    for item in noun1_data:\n",
    "                        p = item['wdLabel']['value']\n",
    "                        if p == different_from_text:\n",
    "                            if item['ps_Label']['value'] == noun2_id['display']['label']['value']:\n",
    "                                return \"No\"\n",
    "                if different_from_text in str(noun2_data):\n",
    "                    log('Different from in n2')\n",
    "                    for item in noun2_data:\n",
    "                        p = item['wdLabel']['value']\n",
    "                        if p == different_from_text:\n",
    "                            if item['ps_Label']['value'] == noun1_id['display']['label']['value']:\n",
    "                                return \"No\"\n",
    "                return \"Yes\"\n",
    "    return \"No\"\n",
    "\n",
    "\n",
    "def try_count(thing_to_count, entity):\n",
    "    if (thing_to_count == 'puppy'):\n",
    "        thing_to_count = 'litter'\n",
    "\n",
    "    log('Entity: ' + str(entity))\n",
    "    log('thing to count: ' + str(thing_to_count))\n",
    "\n",
    "    if entity == 'earth':\n",
    "        entity = thing_to_count\n",
    "        thing_to_count = 'population'\n",
    "\n",
    "    # Find the wikidata id of the entity\n",
    "    entity_ids = get_wikidata_ids(entity)\n",
    "\n",
    "    for entity_id in entity_ids:\n",
    "        log('trying: ' + entity_id['display']['label']['value'])\n",
    "        data = query_wikidata(query_all_properties(entity_id['id']))\n",
    "        if thing_to_count in str(data):\n",
    "            log('Thing to count in data')\n",
    "            for item in data:\n",
    "                ps = item['ps_Label']['value']\n",
    "\n",
    "                if ps.replace('.', '', 1).replace(',', '', 1).isdigit():\n",
    "                    log(\"ps is a number\")\n",
    "                    pq = ps\n",
    "                    ps = item['wdLabel']['value']\n",
    "                else:\n",
    "                    if 'pq_Label' not in item:\n",
    "                        continue\n",
    "                    pq = item['pq_Label']['value']\n",
    "\n",
    "                log('property: ' + str(ps))\n",
    "                log('qualifier: ' + str(pq))\n",
    "\n",
    "                if pq.replace('.', '', 1).replace(',', '', 1).isdigit() and thing_to_count in ps:\n",
    "                    return pq\n",
    "    return None\n",
    "\n",
    "\n",
    "def count_q(doc):\n",
    "    log(\"Count question\")\n",
    "    nouns = list(doc.noun_chunks)\n",
    "\n",
    "    log('Noun chunks: ' + str(nouns))\n",
    "\n",
    "    # Cant find the nouns in the question\n",
    "    if len(nouns) == 0:\n",
    "        return None\n",
    "\n",
    "    if len(nouns) == 1:\n",
    "        nouns = nouns[0].text.lower().replace('how many ', '').split()\n",
    "        if len(nouns) != 2:\n",
    "            thing_to_count = 'population'\n",
    "        else:\n",
    "            thing_to_count = wnl.lemmatize(nouns[1])\n",
    "        entity = wnl.lemmatize(nouns[0])\n",
    "        r = try_count(thing_to_count, entity)\n",
    "        if r is not None:\n",
    "            return r\n",
    "        return try_count(entity, thing_to_count)\n",
    "    else:\n",
    "        thing_to_count = nouns[0].text.lower().replace('how many ', '')\n",
    "        thing_to_count = wnl.lemmatize(thing_to_count)\n",
    "        if len(nouns) == 2:\n",
    "            entity = wnl.lemmatize(remove_article(nouns[1].text.lower()))\n",
    "            r = try_count(thing_to_count, entity)\n",
    "            if r is not None:\n",
    "                return r\n",
    "            return try_count(entity, thing_to_count)\n",
    "        else:\n",
    "            entity = wnl.lemmatize(remove_article(nouns[-1].text.lower()))\n",
    "            r = try_count(thing_to_count, entity)\n",
    "            if r is not None:\n",
    "                return r\n",
    "            r = try_count(entity, thing_to_count)\n",
    "            if r is not None:\n",
    "                return r\n",
    "            entity = wnl.lemmatize(remove_article(nouns[1].text.lower()))\n",
    "            r = try_count(thing_to_count, entity)\n",
    "            if r is not None:\n",
    "                return r\n",
    "            return try_count(entity, thing_to_count)\n",
    "\n",
    "\n",
    "def how_q(doc):\n",
    "    if 'how many' in doc.text.lower():\n",
    "        return count_q(doc)\n",
    "\n",
    "    nouns = list(doc.noun_chunks)\n",
    "\n",
    "    entity = process_noun(wnl.lemmatize(remove_article(nouns[-1].text)))\n",
    "\n",
    "    log('Entity: ' + str(entity))\n",
    "\n",
    "    if doc[-1].pos_ == \"NOUN\":\n",
    "        # if there are two noun chunks, then it has a structure similar to a \"what-question\"\n",
    "        if len(nouns) >= 2:\n",
    "            prop = process_noun(remove_article(nouns[-2].text))\n",
    "        # check if adjective in the last to second position is part of noun chunks\n",
    "        elif doc[-2].pos_ == \"ADJ\" and doc[-2].head == doc[-1] and (doc[-2].text not in nouns[-1].text):\n",
    "            entity = process_noun(wnl.lemmatize(remove_article(doc[-1].text)))\n",
    "            prop = process_noun(doc[-2].text)\n",
    "        # any type of \"how fast\" / \"how strong\" question, except \"how long\"\n",
    "        elif doc[1].pos_ == \"ADV\" or doc[1].pos_ == \"ADJ\" and doc[1].text != \"long\":\n",
    "            prop = process_noun(doc[1].text)\n",
    "        # other questions, assume that entity is at the end of query and the property is the only noun chunk\n",
    "        else:\n",
    "            entity = process_noun(remove_article(doc[-1].text))\n",
    "            prop = process_noun(nouns[-1].text)\n",
    "\n",
    "    elif doc[-1].pos_ == \"VERB\":\n",
    "        if (doc[1].text == \"old\" or doc[1].text == \"long\") and doc[1].head == doc[-1]:\n",
    "            prop = \"life expectancy\"\n",
    "            # people think when they ask \"can\", it means the highest an animal can achieve, so highest lifespan\n",
    "            if (doc[2].text == \"can\"):\n",
    "                prop = \"highest observed lifespan\"\n",
    "        elif doc[1].pos_ == \"ADV\":\n",
    "            prop = process_noun(doc[1].text)\n",
    "        else:\n",
    "            prop = process_noun(doc[-1].text)\n",
    "    elif doc[-1].pos_ == \"ADJ\":\n",
    "        prop = process_noun(doc[-1].text)\n",
    "    elif doc[-1].pos_ == \"ADP\":\n",
    "        print(\"correct\")\n",
    "        prop = process_noun(doc[-2].text)\n",
    "    else:\n",
    "        print(\"sorry, not implemented yet!\")\n",
    "        return nouns\n",
    "\n",
    "    entity = re.sub('adult ', '', entity)\n",
    "    entity = re.sub('birth ', '', entity)\n",
    "    entity = re.sub('newborn ', '', entity)\n",
    "\n",
    "    log('Entity: ' + str(entity))\n",
    "\n",
    "    ans = solver(entity, prop)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def when_q(doc):\n",
    "    nouns = list(doc.noun_chunks)\n",
    "    if \"extinct\" in doc.text:\n",
    "        prop1 = \"end time\"\n",
    "        prop2 = \"extinction date\"\n",
    "    elif \"first\" or \"come to exist\" or \"exist\" or \"start\" in doc.text:\n",
    "        prop1 = \"start time\"\n",
    "        prop2 = None\n",
    "    elif \"born\" in doc.text:\n",
    "        prop1 = \"date of birth\"\n",
    "        prop2 = None\n",
    "    elif \"invented\" in doc.text:\n",
    "        prop1 = \"time of discovery or invention\"\n",
    "        prop2 = None\n",
    "    elif \"die\" in doc.text:\n",
    "        prop1 = \"date of death\"\n",
    "        prop2 = None\n",
    "    noun = remove_article(nouns[0].text)\n",
    "    entity = wnl.lemmatize(noun)\n",
    "    entity = process_noun(entity)\n",
    "    ans = solver(entity, prop1)\n",
    "    if ans == None and prop2 != None:\n",
    "        ans = solver(entity, prop2)\n",
    "    if ans:\n",
    "        if ans[0] == '-':\n",
    "            ans = ans[1:]\n",
    "            if \"-01-01t00:00:00z\" in ans:\n",
    "                ans = ans.replace(\"-01-01t00:00:00z\", \" years BCE\")\n",
    "        else:\n",
    "            ans = datetime.datetime.strptime(ans, \"%Y-%m-%dt%H:%M:%Sz\")\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "diffName = [\"also known as\", \"other names\", \"other word\", \"another name\"]\n",
    "\n",
    "\n",
    "def process_noun(word):\n",
    "\n",
    "    match word:\n",
    "        case 'heavy':\n",
    "            return \"mass\"\n",
    "        case 'fruit type':\n",
    "            return \"has fruit type\"\n",
    "        case 'old ':\n",
    "            return \"highest observed lifespan\"\n",
    "        case 'sound ':\n",
    "            return \"produced sound\"\n",
    "        case 'definition ':\n",
    "            return \"Description\"\n",
    "        case 'pregnant' | 'pregnancy':\n",
    "            return \"gestation period\"\n",
    "        case 'Dodo' | 'dodo':\n",
    "            return \"Raphus cucullatus\"\n",
    "        case \"bonsai tree\" | \"bonsai trees\":\n",
    "            return \"bonsai\"\n",
    "        case 'component' | 'the components':\n",
    "            return \"has parts\"\n",
    "        case 'family':\n",
    "            return \"parent taxon\"\n",
    "        case 'made' | 'produces':\n",
    "            return \"natural product of taxon\"\n",
    "        case \"move\":\n",
    "            return \"locomotion\"\n",
    "        case \"fast\":\n",
    "            return \"speed\"\n",
    "        case \"tall\" | \"high\" | \"big\":\n",
    "            return \"height\"\n",
    "        case \"lactate\" | \"breastfed\":\n",
    "            return \"period of lactation\"\n",
    "        case \"wolf\":\n",
    "            return \"grey wolf\"\n",
    "        case _:\n",
    "            return word\n",
    "\n",
    "\n",
    "def solver(entity, property):\n",
    "    entity = process_noun(entity)\n",
    "    property = process_noun(property)\n",
    "\n",
    "    log(\"entity,property:\" + entity + \", \" + property)\n",
    "\n",
    "    possible_objects = get_wikidata_ids(entity)\n",
    "    possible_properties = get_wikidata_ids(property, True)\n",
    "    # extra = get_synonyms(property)\n",
    "    # for i in extra:\n",
    "    # possible_properties.extend(get_wikidata_ids(i, True))\n",
    "\n",
    "    for object in possible_objects:\n",
    "        for prop in possible_properties:\n",
    "            log('trying: ' + prop['display']['label']['value'] +\n",
    "                ' of ' + object['display']['label']['value'])\n",
    "            ans = query_wikidata(build_query(object['id'], prop['id']))\n",
    "            if ans is not None:\n",
    "                return format_answer(ans)\n",
    "\n",
    "\n",
    "def format_answer(data):\n",
    "    formatted_answers = []\n",
    "\n",
    "    for current_ans in data:\n",
    "        ans = str(current_ans['answerLabel']['value']).capitalize()\n",
    "\n",
    "        unit = ''\n",
    "        if 'unitLabel' in current_ans and current_ans['unitLabel']['value'] != \"1\":\n",
    "            unit = ' ' + current_ans['unitLabel']['value']\n",
    "\n",
    "        # If the answer is a (number > 1) + unit combination, make unit plural\n",
    "        if ans.replace('.', '', 1).replace(',', '', 1).isdigit() and float(ans) > 1.0 and unit != '':\n",
    "            unit += 's'\n",
    "\n",
    "        formatted_answers.append(ans + unit)\n",
    "\n",
    "    final_answer = ''\n",
    "\n",
    "    final_answer += formatted_answers[0]\n",
    "    # It was printing the same value twice if the list has only one item, added if statement\n",
    "    if(len(formatted_answers) > 1):\n",
    "        for ans in formatted_answers[1:-1]:\n",
    "            final_answer += ', ' + ans\n",
    "        final_answer += ', and ' + formatted_answers[-1]\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "def query_wikidata(query):\n",
    "    try:\n",
    "        data = requests.get(SPARQL_ENDPOINT,\n",
    "                            headers=HEADERS,\n",
    "                            params={'query': query, 'format': 'json'}).json()\n",
    "        if 'ASK' in query:\n",
    "            if data['boolean']:\n",
    "                return 'Yes'\n",
    "            else:\n",
    "                return 'No'\n",
    "        else:\n",
    "            data = data['results']['bindings']\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        return 'Error: Query failed: Too many requests?'\n",
    "\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_query(object, property):\n",
    "    q = 'SELECT ?answerLabel ?unitLabel WHERE{wd:'\n",
    "    q += object + ' p:' + property + '?s.?s ps:'\n",
    "    q += property + '?answer. OPTIONAL{?s psv:'\n",
    "    q += property + '?u.?u wikibase:quantityUnit ?unit.}'\n",
    "    q += 'SERVICE wikibase:label{bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\"}}'\n",
    "    return q\n",
    "\n",
    "\n",
    "def build_label_query(obj):\n",
    "    q = 'SELECT ?answerLabel WHERE { wd:'\n",
    "    q += obj + \\\n",
    "        ' skos:altLabel ?answerLabel. FILTER ( lang(?answerLabel) = \"en\" ) }'\n",
    "    return q\n",
    "\n",
    "\n",
    "def build_yes_no_query(object, property):\n",
    "    q = 'ASK { wd:'\n",
    "    q += object + ' wdt:P279 wd:' + property + '. }'\n",
    "    log(q)\n",
    "    return q\n",
    "\n",
    "\n",
    "def query_all_properties(entity):\n",
    "    return '''SELECT ?wdLabel ?ps_Label ?wdpqLabel ?pq_Label {\n",
    "    VALUES (?entity) {(wd:$entity$)}\n",
    "    ?entity ?p ?statement .\n",
    "    ?statement ?ps ?ps_ .\n",
    "    ?wd wikibase:claim ?p.\n",
    "    ?wd wikibase:statementProperty ?ps.\n",
    "    OPTIONAL {\n",
    "    ?statement ?pq ?pq_ .\n",
    "    ?wdpq wikibase:qualifier ?pq .\n",
    "    }\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "    } ORDER BY ?wd ?statement ?ps_'''.replace('$entity$', entity)\n",
    "\n",
    "\n",
    "def get_wikidata_ids(query, isProperty=False):\n",
    "    params = {'action': 'wbsearchentities',\n",
    "              'language': 'en',\n",
    "              'format': 'json',\n",
    "              'search': query}\n",
    "    if isProperty:\n",
    "        params['type'] = 'property'\n",
    "\n",
    "    return requests.get(API_ENDPOINT, params).json()['search']\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
